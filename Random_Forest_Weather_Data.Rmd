---
title: "Random Forest Weather Data"
author: "Samantha Bothwell"
date: "6/11/2019"
output: pdf_document
---
The random forest algorithm is a form of supervised learning. Random Forests are a form of decision trees. In the algorithm, trees are built by choosing a random subset of variables and then the model averages the predicitons of all the trees.

The random forest algorithm is most often used when there is a mixture of numeric and factor variables. Before using the random forest algorithm, here are some pros and cons to consider:

\textbf{Pros:}
\begin{itemize}
  \item You can obtain a reliable variable importance plot that ranks the variables by how helpful they are in predicting the response.
  \item The algorithm is effectively able to fill in missing information based on what information is present.
  \item The estimate for the generalization error progresses as the forest builds, and is unbiased.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
  \item This is a black box algorithm - meaning, though it is powerful for predicting, it is difficult to interpret.
  \item When working with a dataset with significant noise, the algorithm will overfit.
  \item Variable importance is biased towards categorical variables with more levels. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load packages}
# Load important packages
library(tidyr)
library(randomForest)
library(dplyr)
library(party)
library(rpart)
library(rpart.plot)
library(rattle)
library(AUCRF)
library(ggplot2)
library(pdp)
```

```{r clean data}
# First we read in the data and clean it
setwd("/Users/sbothwell/Desktop/DATAS/My_material")
dat = read.csv("Weather.csv", stringsAsFactors = FALSE)

# Remove first column
dat = dat[,-1]

# Rename columns
colnames(dat) = c("DateTime_MST","Temp_degF","Humidity_Pct","DewPt_degF","Wind_mph",
                  "WindDir_degNorth","Gust_mph","GustDir_degNorth","Pressure_Hg",
                  "Solar_WatPerSqM","Percipitation_in")

# Split date and time
dat = dat %>% separate(`DateTime_MST`, c("Date","Time_MST"), " ")

# Split date
dat = dat %>% separate(Date, c("Year","Month","Day"), "-")

# Split time to make numeric
dat = dat %>% separate(`Time_MST`, c("Time", "Minute"), ":")
dat = dat[,-5]
dat$Time = as.numeric(dat$Time)

# Change numeric month to character
dat$Month = as.numeric(dat$Month)
dat$Month = month.abb[dat$Month]

# remove any NA's from the dataset
dat = dat[complete.cases(dat),]

# all varibles must be numeric or factor for RF to work
dat$Year = as.factor(dat$Year)
dat$Month = as.factor(dat$Month)
dat$Day = as.factor(dat$Day)
```

Now that our data is clean, let's walk through using the Random Forest Algorithm.
First we will make data sets for testing and training. These are random samples from our original data. We will use the training data to make the model and the testing data to evaluate how effective our model is at making predictions. For this data, I will make my training data 75% of the original data set and testing data the other 25% of the original data set. 
```{r make data sets}
set.seed(2019) # set seed for reproducible results

## 75% of the sample size
smp_size <- floor(0.75 * nrow(dat))
train_ind <- sample(seq_len(nrow(dat)), size = smp_size)

train <- dat[train_ind, ]
test <- dat[-train_ind, ]
```
For the model we will try to predict the temperature based on the remaining variables
```{r train the model}
# Random Forest algorithm
model = randomForest(`Temp_degF`~., data = train)
```

We can view one of the trees from the Forest
```{r print a tree}
# make a function to print certain text in tree nodes
nodefun = function(x, labs, digits, varlen){
  paste("Avg Temp:", round(x$frame$yval,2), "\nn =", x$frame$n, "  ", 
        "\nPct =", round(x$frame$n/x$frame$n[1]*100,2),"%")
}

# plot decision tree
tree = rpart(model, train)
fancyRpartPlot(tree, palettes=c("Reds"), main="Decision Tree Graph", sub="", yesno = 2, node.fun = nodefun)
```


