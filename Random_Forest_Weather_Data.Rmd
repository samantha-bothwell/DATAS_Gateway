---
title: "Random Forest Weather Data"
author: "Samantha Bothwell"
date: "6/11/2019"
output: pdf_document
---


## Random Forest Background

The random forest algorithm is a form of supervised learning. Random Forests are a form of decision trees. In the algorithm, trees are built by choosing a random subset of variables and then the model averages the predicitons of all the trees. 

The random forest algorithm is most often used when there is a mixture of numeric and factor variables. Before using the random forest algorithm, here are some pros and cons to consider:

\textbf{Pros:}
\begin{itemize}
  \item You can obtain a reliable variable importance plot that ranks the variables by how helpful they are in predicting the response.
  \item The algorithm is effectively able to fill in missing information based on what information is present.
  \item The estimate for the generalization error progresses as the forest builds, and is unbiased.
\end{itemize}

\textbf{Cons:}
\begin{itemize}
  \item This is a black box algorithm - meaning, though it is powerful for predicting, it is difficult to interpret.
  \item When working with a dataset with significant noise, the algorithm will overfit.
  \item Variable importance is biased towards categorical variables with more levels. 
  \item The algorithm performs poorly with predicting future data if there is any seasonality. 
\end{itemize}

The takeaway here is that the algorithm will perform poorly with time series data. This will be shown in the document.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r load packages, include=FALSE}
# Load important packages
library(tidyr)
library(randomForest)
library(AUCRF)
library(dplyr)
library(party)
library(rpart)
library(rpart.plot)
library(rattle)
library(ggplot2)
library(pdp)
library(ROCR)
library(MLmetrics)
```

## Clean Data
The data we will be using was taken from http://www.atmos.colostate.edu/fccwx/fccwx_latest.php, a record of climate observations for Fort Collins, CO. The data was taken hourly from June 2018 - June 2019. Later we will look at data from June 2014 - June 2019. We will remove the first column that represents the time the data point was taken. 
```{r clean data}
# First we read in the data and clean it
setwd("/Users/sbothwell/Desktop/DATAS/My_material")
dat = read.csv("Weather.csv", stringsAsFactors = FALSE)

# Remove first two columns
dat = dat[,-c(1,2)]

# Rename columns
colnames(dat) = c("Temp_degF","Humidity_Pct","DewPt_degF","Wind_mph",
                  "WindDir_degNorth","Gust_mph","GustDir_degNorth","Pressure_Hg",
                  "Solar_WatPerSqM","Percipitation_in")

# remove any NA's from the dataset
dat = dat[complete.cases(dat),]
```

## Analysis - Filling in missing data
Now that our data is clean, let's walk through using the Random Forest Algorithm.
First, we will make data sets for testing and training. These are random samples from our original data. We will use the training data to make the model and the testing data to evaluate how effective our model is at making predictions. For this data, I will make my training data 75% of the original data set and testing data the other 25% of the original data set. 
```{r make data sets}
set.seed(2019) # set seed for reproducible results

## 75% of the sample size
smp_size <- floor(0.75 * nrow(dat))
train_ind <- sample(seq_len(nrow(dat)), size = smp_size)

train <- dat[train_ind, ]
test <- dat[-train_ind, ]
```

For the model we will try to predict the temperature based on the remaining variables.
```{r train the model, cache = TRUE}
# Random Forest algorithm
model = randomForest(`Temp_degF`~., data = train)
```

### Visuals
There are many visuals we can make from Random Forest to understand our data better. The visuals I will give examples of are:
\begin{itemize}
  \item Decision Tree
  \item Variable Importance Plot
  \item Partial Plot
\end{itemize}

#### Decision Tree
We can view one of the trees from the Forest.
```{r print a tree}
# make a function to print certain text in tree nodes
nodefun = function(x, labs, digits, varlen){
  paste("Avg Temp:", round(x$frame$yval,2), "\nn =", x$frame$n, "  ", 
        "\nPct =", round(x$frame$n/x$frame$n[1]*100,2),"%")
}

# plot decision tree
tree = rpart(model, train)
fancyRpartPlot(tree, palettes=c("Reds"), main="Decision Tree Graph", sub="",
               yesno = 2, node.fun = nodefun)
```



#### Variable Importance Plot
With the AUCRF package (Area Under the Curve Random Forest)
```{r}
IncNodePur = model$importance[1:9]
names = colnames(train)[c(2:10)]

Imp = as.data.frame(cbind(names, IncNodePur))
Imp$IncNodePur = as.numeric(as.character(Imp$IncNodePur))
Imp$names = factor(Imp$names, levels = Imp$names[order(Imp$IncNodePur)])
Imp = Imp[order(-IncNodePur),]

p = ggplot(Imp, aes(IncNodePur, names)) +
  geom_point(size = 4, shape = 16, col = "turquoise") +
  xlim(3000, 1500000) + ylab("Predictor") + labs(title = "Variable Importance Plot") + 
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + 
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 12, face = "bold"))
p
```

Based on the variable importance plot we see that Month is the most important variable in telling us about what the temperature will be. The next two important variables are the Dew Point (in $^oF$) and the Humidity Perctentage. 

#### Partial Plots

### Evaluate Model
Now let's evaluate how effective our model is with mean squared error:
```{r evaluate model}
predictions = predict(model, test)

MSE(predictions, test$Temp_degF)
```
For this model, the mean squared error is \textbf{4.71}

```{r plot data}
# plot predictions vs observed temperatures
ggplot(data = NULL, aes(x=predictions, y=test$Temp_degF)) +
  geom_point(col = "mediumorchid") +
  xlab("Predicted") + ylab("Observed") +
  geom_abline(intercept = 0, slope = 1, color = "navy", size = 1.5) + 
  geom_text(data=data.frame(x=92,y=85), aes(x, y), label="y = x", color="navy", size = 5)
```





\newpage

## Analysis - Future Predictions
```{r, include = FALSE}
# First we read in the data and clean it
setwd("/Users/sbothwell/Desktop/DATAS/My_material")
dat = read.csv("Weather.csv", stringsAsFactors = FALSE)

# Remove first two columns
dat = dat[,-c(1,2)]

# Rename columns
colnames(dat) = c("Temp_degF","Humidity_Pct","DewPt_degF","Wind_mph",
                  "WindDir_degNorth","Gust_mph","GustDir_degNorth","Pressure_Hg",
                  "Solar_WatPerSqM","Percipitation_in")

# remove any NA's from the dataset
dat = dat[complete.cases(dat),]
```
So we've seen that random forest is great for filling in missing data, but how about future predictions? In general, random forests don't provide a good fit when there's seasonality in time series data.

```{r}
# make testing and training data sets
N = ceiling(0.75*nrow(dat))
train = dat[1:N-1,] #First 75% of the data
test = dat[N:nrow(dat),] #Final 25% of the data
```

```{r, cache = TRUE}
# Random Forest algorithm
model = randomForest(`Temp_degF`~., data = train)
```

```{r}
predictions = predict(model, test)

MSE(predictions, test$Temp_degF)
```
Our mean squared error in this case is significantly larger than our previous test! In this case our mean squared error is 78.59, compared to 4.71 from before. 

```{r}
# plot predictions vs observed temperatures
ggplot(data = NULL, aes(x=predictions, y=test$Temp_degF)) +
  geom_point(col = "mediumorchid") +
  xlab("Predicted") + ylab("Observed") +
  geom_abline(intercept = 0, slope = 1, color = "navy", size = 1.5) + 
  geom_text(data=data.frame(x=92,y=85), aes(x, y), label="y = x", color="navy", size = 5)
```
So the predicted value is consistently higher than the observed value. 

In general, the Random Forest algorithm will provide an average result and can not predict future results (extrapolate) becuase it does not understand data with seasonality. 

